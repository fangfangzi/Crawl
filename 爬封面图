import re
import requests
from bs4 import BeautifulSoup
import multiprocessing as mp
from urllib.request import urlretrieve

depth = 10

def gethtmls(url):
    try:
        r =requests.get(url)
        r.encoding = r.apparent_encoding
        return r.text
    except:
        print('爬取网页内容失败')
       
def get_preview_list():
    preview_urls_list = []
    for i in range(depth):
        url = 'https://www.zhainanfulishe.com/nvshen/' + 'page/' + str(i+1)
        preview_urls_list.append(url)
    return preview_urls_list

def getdetailedurls():
    detailed_urls_list = []
    for url in preview_urls_list:        
        try:
            html = gethtmls(url)
            reg3 = r'<a href=".*?" title=".*?" class=".*?">'
            permenentlist = re.findall(reg3,html)
            for each in permenentlist:
                detailed_urls_list.append(each.split('"')[1])
                print('Save')
        except:
            print('Faile')
            continue
    return detailed_urls_list
    
    def get_what_you_want():
    jpg_list = []
    for url in detailed_urls_list:
        try:
            html = gethtmls(url)
            soup = BeautifulSoup(html,'lxml')
            l1 = soup.find_all('img',class_ = re.compile("alignnone size-full .*?"))
            for i in range(len(l1)):
                jpg_list.append(l1[i].attrs['src'])
        except:
            continue
    return jpg_list
    
    def downloadfunction(jpglist):
    root = 'E:\学习\爬虫\default\爬虫\吹牛逼练习\流鼻血了\\'
    for each in jpglist:
        path = root + each.split('/')[-1] + '.jpg'
        urlretrieve(each,path)
    
    def main():
    preview_urls_list = get_preview_list()
    detailed_urls_list = getdetailedurls()
    jpglist = get_what_you_want()
    downloadfunction(jpglist)
